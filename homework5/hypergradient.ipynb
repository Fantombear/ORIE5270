{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.extmath import safe_sparse_dot, squared_norm\n",
    "from scipy.misc import comb, logsumexp \n",
    "from sklearn.linear_model.logistic import _multinomial_grad_hess\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into traning and test datasets\n",
    "Here I add one more step to split the training data into training data and validation data instead of using only training set and testing set in the original notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 30000\n",
    "valid_samples = 10000\n",
    "test_samples = 10000\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples+valid_samples, test_size=test_samples, random_state=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=test_samples, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "Y_train = np.zeros((len(y_train), 10))\n",
    "for i,j in enumerate(y_train):\n",
    "    Y_train[i, int(j)] = 1\n",
    "Y_valid = np.zeros((len(y_valid), 10))\n",
    "for i,j in enumerate(y_valid):\n",
    "    Y_valid[i, int(j)] = 1\n",
    "Y_test = np.zeros((len(y_test), 10))\n",
    "for i,j in enumerate(y_test):\n",
    "    Y_test[i, int(j)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    x=x-np.max(x,axis=1,keepdims=True) #avoid nan\n",
    "    f=(np.exp(x)/np.sum(np.exp(x),axis=1,keepdims=True))\n",
    "    return f\n",
    "def create_id_matrix(Y):\n",
    "    n=Y.shape[0]\n",
    "    t=np.zeros((n,10))\n",
    "    for i in range(10):\n",
    "        t[np.where(Y==i)[0],i]=1\n",
    "    return t\n",
    "def gradient(X,Y,w,alpha=0):\n",
    "    y=logistic(np.dot(X,w))\n",
    "    g=np.dot((y-Y).T,X).T+alpha*w\n",
    "    return g\n",
    "def hessian(X,Y,w,alpha=0):\n",
    "    y=logistic(np.dot(X,w))\n",
    "    xy=[]\n",
    "    xyx=[]\n",
    "    for i in range(10):\n",
    "        xy_i=y[:,i].reshape((len(y),-1))*X\n",
    "        xyx_i=safe_sparse_dot(xy_i.T,X)\n",
    "        xy.append(xy_i)\n",
    "        xyx.append(xyx_i)\n",
    "    xy=np.concatenate(xy,axis=1)\n",
    "    h=-scipy.linalg.block_diag(*xyx)+safe_sparse_dot(xy.T,xy)+alpha\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.36 s\n"
     ]
    }
   ],
   "source": [
    "w=np.random.rand(7840).reshape((784,10))\n",
    "%time hessian(X_train[:1001,:],Y_train[:1001,:],w,alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = Y_train.shape[1]\n",
    "n_features = X_train.shape[1]\n",
    "d=n_classes*n_features\n",
    "#initialize Z_0, V_0, W_0\n",
    "Z_t=np.zeros((d*2,2))\n",
    "V_t=np.zeros((d,1))\n",
    "W_t=np.zeros((d,1))\n",
    "index = np.random.randint(X_train.shape[0], size=1000)\n",
    "batch_X = X_train[index,:]\n",
    "batch_Y = Y_train[index,:]\n",
    "grad=gradient(batch_X,batch_Y,W_t.reshape((n_features,n_classes))).reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7840, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.random.randint(X_train.shape[0], size=1000)\n",
    "batch_X = X_train[index,:]\n",
    "batch_Y = Y_train[index,:]\n",
    "grad=gradient(batch_X,batch_Y,W_t)\n",
    "#S_t update\n",
    "V_t = momentum * V_t + lr * grad\n",
    "W_t= W_t - V_t\n",
    "#calculate B_t\n",
    "B11 = grad.ravel \n",
    "#calculate A_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypergradient(X_train,Y_train,X_valid,Y_valid,w,alpha,lr,momentum=0.9):\n",
    "    n_classes = Y_train.shape[1]\n",
    "    n_features = X_train.shape[1]\n",
    "    d=n_classes*n_features\n",
    "    #initialize Z_0, V_0, W_0\n",
    "#     V_t=np.zeros((d,1))\n",
    "#     W_t=np.zeros((d,1))\n",
    "    #    \n",
    "    Z_t=np.zeros((d*2,2))\n",
    "    V_t=np.random.rand(d,1)\n",
    "    W_t=np.random.rand(d,1)\n",
    "    for t in range(100):\n",
    "        print(\"iterations: \"+str(t))\n",
    "        index = np.random.randint(X_train.shape[0], size=1000)\n",
    "        batch_X = X_train[index,:]\n",
    "        batch_Y = Y_train[index,:]\n",
    "        #calculate batch raveled gradient at w_(t-1)\n",
    "        grad = gradient(batch_X,batch_Y,W_t.reshape((n_features,n_classes)),alpha).reshape((-1,1))    \n",
    "        #calculate batch hessian at w_(t-1)\n",
    "        hess = hessian(batch_X,batch_Y,W_t.reshape((n_features,n_classes)),alpha)\n",
    "        #S_t update\n",
    "        V_t = momentum * V_t + lr * grad\n",
    "        W_t = W_t - V_t\n",
    "        #calculate B_t\n",
    "        B11 = grad\n",
    "        B12 = lr * W_t.reshape((-1,1))\n",
    "        B21 = - B11\n",
    "        B22 = - B12\n",
    "        B_t = np.block([[B11,B12],[B21,B22]])\n",
    "        #calculate A_t\n",
    "        A11 = momentum * np.identity(d) \n",
    "        A12 = lr * hess\n",
    "        A21 = -A11\n",
    "        A22 = np.identity(d) - A12\n",
    "        A_t = np.block([[A11,A12],[A21,A22]])\n",
    "        #Z_t update\n",
    "        Z_t = safe_sparse_dot(A_t,Z_t)+B_t\n",
    "        print(sum(W_t))\n",
    "        #end for \n",
    "    #evaluate gradient at W_T\n",
    "    grad = gradient(X_valid, Y_valid, W_t.reshape((n_features,n_classes)), alpha).reshape((-1,1))\n",
    "    hypergrad=safe_sparse_dot(grad.T,Z_t[d:,:])\n",
    "    return hypergrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 0\n",
      "[ 405.26996007]\n",
      "iterations: 1\n",
      "[-2769.39540049]\n",
      "iterations: 2\n",
      "[-5623.82482959]\n",
      "iterations: 3\n",
      "[-8187.18749094]\n",
      "iterations: 4\n",
      "[-10486.02669868]\n",
      "iterations: 5\n",
      "[-12544.49595894]\n",
      "iterations: 6\n",
      "[-14384.57379721]\n",
      "iterations: 7\n",
      "[-16026.25927786]\n",
      "iterations: 8\n",
      "[-17487.74995117]\n",
      "iterations: 9\n",
      "[-18785.6038072]\n",
      "iterations: 10\n",
      "[-19934.88667381]\n",
      "iterations: 11\n",
      "[-20949.30636709]\n",
      "iterations: 12\n",
      "[-21841.33478468]\n",
      "iterations: 13\n",
      "[-22622.31902572]\n",
      "iterations: 14\n",
      "[-23302.58252363]\n",
      "iterations: 15\n",
      "[-23891.51708923]\n",
      "iterations: 16\n",
      "[-24397.66668118]\n",
      "iterations: 17\n",
      "[-24828.80364725]\n",
      "iterations: 18\n",
      "[-25191.99811307]\n",
      "iterations: 19\n",
      "[-25493.68113419]\n",
      "iterations: 20\n",
      "[-25739.70217207]\n",
      "iterations: 21\n",
      "[-25935.38140399]\n",
      "iterations: 22\n",
      "[-26085.55733131]\n",
      "iterations: 23\n",
      "[-26194.63010856]\n",
      "iterations: 24\n",
      "[-26266.60097799]\n",
      "iterations: 25\n",
      "[-26305.10815949]\n",
      "iterations: 26\n",
      "[-26313.45951468]\n",
      "iterations: 27\n",
      "[-26294.66227484]\n",
      "iterations: 28\n",
      "[-26251.45009671]\n",
      "iterations: 29\n",
      "[-26186.3076863]\n",
      "iterations: 30\n",
      "[-26101.49320924]\n",
      "iterations: 31\n",
      "[-25999.05868667]\n",
      "iterations: 32\n",
      "[-25880.86855768]\n",
      "iterations: 33\n",
      "[-25748.61657303]\n",
      "iterations: 34\n",
      "[-25603.84117027]\n",
      "iterations: 35\n",
      "[-25447.93946662]\n",
      "iterations: 36\n",
      "[-25282.17999386]\n",
      "iterations: 37\n",
      "[-25107.71428839]\n",
      "iterations: 38\n",
      "[-24925.58743917]\n",
      "iterations: 39\n",
      "[-24736.74768744]\n",
      "iterations: 40\n",
      "[-24542.05516319]\n",
      "iterations: 41\n",
      "[-24342.28983621]\n",
      "iterations: 42\n",
      "[-24138.15875209]\n",
      "iterations: 43\n",
      "[-23930.30261762]\n",
      "iterations: 44\n",
      "[-23719.30179399]\n",
      "iterations: 45\n",
      "[-23505.68175092]\n",
      "iterations: 46\n",
      "[-23289.91803042]\n",
      "iterations: 47\n",
      "[-23072.44076393]\n",
      "iterations: 48\n",
      "[-22853.63878332]\n",
      "iterations: 49\n",
      "[-22633.863362]\n",
      "iterations: 50\n",
      "[-22413.43161944]\n",
      "iterations: 51\n",
      "[-22192.62961952]\n",
      "iterations: 52\n",
      "[-21971.71518997]\n",
      "iterations: 53\n",
      "[-21750.92048819]\n",
      "iterations: 54\n",
      "[-21530.4543361]\n",
      "iterations: 55\n",
      "[-21310.50434488]\n",
      "iterations: 56\n",
      "[-21091.23884844]\n",
      "iterations: 57\n",
      "[-20872.80866279]\n",
      "iterations: 58\n",
      "[-20655.34868705]\n",
      "iterations: 59\n",
      "[-20438.97936019]\n",
      "iterations: 60\n",
      "[-20223.80798666]\n",
      "iterations: 61\n",
      "[-20009.92994249]\n",
      "iterations: 62\n",
      "[-19797.4297728]\n",
      "iterations: 63\n",
      "[-19586.38219031]\n",
      "iterations: 64\n",
      "[-19376.85298387]\n",
      "iterations: 65\n",
      "[-19168.89984509]\n",
      "iterations: 66\n",
      "[-18962.57312035]\n",
      "iterations: 67\n",
      "[-18757.91649496]\n",
      "iterations: 68\n",
      "[-18554.96761562]\n",
      "iterations: 69\n",
      "[-18353.75865659]\n",
      "iterations: 70\n",
      "[-18154.31683481]\n",
      "iterations: 71\n",
      "[-17956.66487837]\n",
      "iterations: 72\n",
      "[-17760.8214527]\n",
      "iterations: 73\n",
      "[-17566.80154814]\n",
      "iterations: 74\n",
      "[-17374.61683249]\n",
      "iterations: 75\n",
      "[-17184.27597158]\n",
      "iterations: 76\n",
      "[-16995.78492078]\n",
      "iterations: 77\n",
      "[-16809.14719014]\n",
      "iterations: 78\n",
      "[-16624.36408538]\n",
      "iterations: 79\n",
      "[-16441.434927]\n",
      "iterations: 80\n",
      "[-16260.35724954]\n",
      "iterations: 81\n",
      "[-16081.12698257]\n",
      "iterations: 82\n",
      "[-15903.73861532]\n",
      "iterations: 83\n",
      "[-15728.18534618]\n",
      "iterations: 84\n",
      "[-15554.4592186]\n",
      "iterations: 85\n",
      "[-15382.55124457]\n",
      "iterations: 86\n",
      "[-15212.45151669]\n",
      "iterations: 87\n",
      "[-15044.14931009]\n",
      "iterations: 88\n",
      "[-14877.63317483]\n",
      "iterations: 89\n",
      "[-14712.89101993]\n",
      "iterations: 90\n",
      "[-14549.91018949]\n",
      "iterations: 91\n",
      "[-14388.67753191]\n",
      "iterations: 92\n",
      "[-14229.17946256]\n",
      "iterations: 93\n",
      "[-14071.40202068]\n",
      "iterations: 94\n",
      "[-13915.33092096]\n",
      "iterations: 95\n",
      "[-13760.9516003]\n",
      "iterations: 96\n",
      "[-13608.2492601]\n",
      "iterations: 97\n",
      "[-13457.20890467]\n",
      "iterations: 98\n",
      "[-13307.81537587]\n",
      "iterations: 99\n",
      "[-13160.05338457]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -1.90762522e+77,  -3.76515583e+75]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypergradient(X_train,Y_train,X_valid,Y_valid,w,alpha=0.1,lr=0.01,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_loss(X, Y, w, alpha=0):\n",
    "    n_classes = Y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "    fit_intercept = (w.size == n_classes * (n_features + 1))\n",
    "    grad = np.zeros((n_classes, n_features + bool(fit_intercept)),\n",
    "                    dtype=X.dtype)\n",
    "    loss, p, w = loss_function(X, Y, w,alpha)\n",
    "    diff = (p - Y)\n",
    "    grad[:, :n_features] = safe_sparse_dot(diff.T, X)\n",
    "    grad[:, :n_features] += alpha * w\n",
    "    if fit_intercept:\n",
    "        grad[:, -1] = diff.sum(axis=0)\n",
    "    return loss, grad.ravel(), p\n",
    "\n",
    "def hessian_loss(X, Y, w, alpha=0):\n",
    "    n_features = X.shape[1]\n",
    "    n_classes = Y.shape[1]\n",
    "    fit_intercept = w.size == (n_classes * (n_features + 1))\n",
    "\n",
    "    loss, grad, p = grad_loss(X, Y, w,alpha)\n",
    "    def hessp(v):\n",
    "        v = v.reshape(n_classes, -1)\n",
    "        if fit_intercept:\n",
    "            inter_terms = v[:, -1]\n",
    "            v = v[:, :-1]\n",
    "        else:\n",
    "            inter_terms = 0\n",
    "        r_yhat = safe_sparse_dot(X, v.T)\n",
    "        r_yhat += inter_terms\n",
    "        r_yhat += (-p * r_yhat).sum(axis=1)[:, np.newaxis]\n",
    "        r_yhat *= p\n",
    "        hessProd = np.zeros((n_classes, n_features + bool(fit_intercept)))\n",
    "        hessProd[:, :n_features] = safe_sparse_dot(r_yhat.T, X)\n",
    "        hessProd[:, :n_features] += v * alpha\n",
    "        if fit_intercept:\n",
    "            hessProd[:, -1] = r_yhat.sum(axis=0)\n",
    "        return hessProd.ravel()\n",
    "    w_copy = w.copy()\n",
    "    w_copy[-1] = 0.0\n",
    "    return grad, hessp, w_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_intercept = True\n",
    "def sgd(momentum=0.9, lr=0.01, batch_size=1001, alpha=0.1, maxepoch=50, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Real-time forward-mode algorithm using stochastic gradient descent with constant learning \n",
    "    rate. Observe that you should only find the optimal learning rate (lr), and \n",
    "    penalty parameter (alpha). \n",
    "    \n",
    "    We use the SGD with momentum, which is defined here: \n",
    "      \n",
    "    \"\"\"\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
